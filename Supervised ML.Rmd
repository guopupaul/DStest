---
title: "Supervised ML"
output:
  html_document:
    df_print: paged
  word_document: default
---
### Using Basketball boxscore elements and machine learning to identify winning team performance targets

## Introduction
The fundamental question facing basketball teams in competition is "What do we have to do to win?". Typically, the coach's philosophy, gameplan, team member skills/abilities, overall team offensive performance and overall team defensive performance combine in varying ways to determine who wins. Data Analytics in professional levels of basketball has rapidly grown to quantify these factors in very sophisticated ways. The insights derived are actively used to modify style of play and performance targets to enable more likely wins.

I'm an assitant coach one level below professional basketball. Applying current cutting edge techniques in basketball analytics at my level is problematic for two reasons:
1. the resourcing to collect and analyse the required detailed data in our league is not readily available, and
2. the direct applicability of insights from professional basketball to our league is uncertain.

What is readily available in our league is a detailed boxscore for each game. This will be the data source. 135 games were played in a season with 135 "win" boxscores and 135 "loss" boxscores. there are no NA values.

##  Aim
To add some rigor to choosing an optimal style of play and setting effective performance targets, this paper will use a set of game boxscores and machine learning techniques to investigate and model key performance targets for a winning game. 

## Method
1. Investigate data for outlier "win" and"loss" scores
2. Using PCA, reduce the number of dependent variables to avoid overfitting and create a final model which would be easier to use
3. Using machine learning, develop a model and evaluate its performance
4. Benchmark the logistic regression model against a range of other classification models
4. Create a function that uses target values for the key performance variables to predict the likelihood of a "win" 

## The data
Load the following packages:
```{r message = FALSE}
library(tidyverse)
library(mlr3verse)
library(readxl)
library("FactoMineR")
library("factoextra")
library("corrplot")
library(outliers)
```
Here is the data read in:
```{r}
boxscores <- read_excel("YL1 2019.xlsx", sheet = 2, range = "A1:Y271",col_names = TRUE)
glimpse(boxscores)
```
## Is this a normal distribution? Are there outliers that should be excluded from analysis?
```{r}
#histogram of all final scores
hist(boxscores$TP,breaks = 30,xlim=c(25,150),ylim=c(0,20),col="gray")
qqnorm(boxscores$TP)
qqline(boxscores$TP)
summary(boxscores$TP)
```
The boxscores approximate normal distributions. There appear to be two scores at the high end that are outliers. Inter quartile ranges werev used to confirm if they were actual outliers.
```{r}
#gamescores too high
TP_quartiles <- quantile(boxscores$TP)
upper_th <- TP_quartiles[4] + 
   1.5 * (TP_quartiles[4] - TP_quartiles[2])
houtliers <- sort(unique(boxscores$TP[boxscores$TP > upper_th]))
houtliers
#gamescores too low
lower_th <- TP_quartiles[2] -
   1.5 * (TP_quartiles[4] - TP_quartiles[2])
loutliers <- sort(unique(boxscores$TP[boxscores$TP < lower_th]))
loutliers
```
Gamescores of 127 and 133 are then excluded
```{r}
boxscores <- boxscores %>% filter(TP != 127, 133)
glimpse(boxscores)
```

## Data pre-processing
To prepare a data frame with required numeric variables for analysis, we first discount variables that has no correlation connection to a win-loss result for a single game.. This includes:
ROUND, TEAM, NAME, MIN
Some variables are aggregates of other variables or calculations based on other variables. They were also removed:
"3P%", "2P%", "FT%", TR, PTOS, PITP, "2CPS", TP
```{r}
#remove unneeded categorical variables and aggregate variables
boxscores.trim <- boxscores %>% select(-c(RND, TEAM, "3P%", "2P%", "FT%", TR, PTOS, PITP, "2CPS", MIN, TP)) %>%
#convert RESULT to a binomial variable
mutate(RESULT = ifelse(RESULT == "WIN", 1, 0))
#coerce to  a numeric data frame
boxscores.trim <- sapply(boxscores.trim, as.numeric)
```

## Feature selection
The matrix "boxscores.trim" has 14 variables. A model with 14 inputs is not helpful to coaches, and also may lead to an overfitted model. Some variables may play a key contributing role more than others. PrincipalComponent Analysis was used to assess this.
# Principal Component Analysis
```{r}
# remove response variable
boxscores.pca <- boxscores.trim[, 1:13]
res.pca <- PCA(boxscores.pca, graph = FALSE)
#note that the function 'PCA' automatically standardises the data
eig.val <- get_eigenvalue(res.pca)
eig.val
```
An eigenvalue > 1 indicates that PCs account for more variance than accounted by one of the original variables in standardized data. This is commonly used as a cutoff point for which PCs are retained. This holds true only when the data are standardized. The function "PCA" does this automatically. 

In this case, 5 dimensions > 1 and cover 70% of the variance.
An alternative method to determine the number of principal components is to look at a Scree Plot, which is the plot of eigenvalues ordered from largest to the smallest. The number of component is determined at the point, beyond which the remaining eigenvalues are all relatively small and of comparable size
```{r}
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 25))
```
5 dimensions still looks right.
The quality of representation of the variables is called cos2 (square cosine, squared coordinates). You can visualize the cos2 of variables on all the dimensions using the corrplot package:
```{r}
var <- get_pca_var(res.pca)
var
corrplot(var$cos2, is.corr=FALSE)
```
Itâ€™s possible to use the function corrplot() to highlight the most contributing variables for each dimension:
```{r}
corrplot(var$contrib, is.corr=FALSE) 
#a visualisation
fviz_contrib(res.pca, choice = "var", axes = 1:5, top = 13)
```
The red dashed line on the graph above indicates the expected average contribution. If the contribution of the variables were uniform, the expected value would be 1/length(variables) = 1/13. For a given component, a variable with a contribution larger than this cutoff could be considered as important in contributing to the component.
Although 2PM is below the cutoff, we will include it for practical reasons
```{r}
Boxscores <- boxscores.trim %>% subset(TRUE, c(1:6, 8, 10, 12, 14)) %>%
data.frame()
summary(Boxscores)

```
## Machine Learning
```{r setup}
set.seed(88)
if (!interactive()) {
  lgr::get_logger("mlr3")$set_threshold("warn")
}
```
# Build a task
```{r}
#first make response variable a factor for classification purposes
Boxscores$RESULT <- as.factor(Boxscores$RESULT)
Boxscores_tsk <- TaskClassif$new(id = "Boxscores", backend = Boxscores, target = "RESULT", positive = "1")
Boxscores_tsk
```
# Define training(75%) and test(25%) sets
```{r}
train_set <- sample(Boxscores_tsk$nrow, 0.75 * Boxscores_tsk$nrow)
test_set <- setdiff(seq_len(Boxscores_tsk$nrow), train_set)
```
# Define a logistic regression learner
```{r}
# Check available learners
as.data.table(mlr_learners)
#Define an mlr3 Learner
lrn <- lrn("classif.log_reg", predict_type = "prob") # shorter way
```

# Train the Learner on the Task

Train the previously defined learner on the `Boxscores_tsk` task.
```{r}
learner <- lrn$train(task = Boxscores_tsk, row_ids = train_set)
```

The learner now contains the trained model:
```{r}
class(lrn$model)
summary(lrn$model)
```
### Make Predictions

Use the learner that now stores the model to predict the label on the `Boxscores.test` data.
```{r}
# Learner
pred <- lrn$predict(task = Boxscores_tsk, row_ids = test_set)
#confusion matrix
pred$confusion
autoplot(pred)
```

# choose different measures to score the prediction
```{r}
mlr_measures
msrs = lapply(c("classif.auc", "classif.precision", "classif.recall"), msr)
pred$score(measures = msrs)
```
## Resampling
To automate the test train split we can use resample. Therefore we need a resampling strategy.
```{r}
# show available resampling objects
mlr_resamplings

# load cross validation
rds <- rsmp("cv")
# check how many folds
rds$param_set$values
```

We want to do a 10-fold cross-validation:
```{r}
# change the folds parameter of the resampling description object
rds$param_set$values$folds = 10

# execute resampling
res <- resample(task = Boxscores_tsk, learner = lrn, resampling = rds)

# calculate performance measures with previously defined measures
res$score(msrs)
res$aggregate(msrs)
```

## Benchmarking
We are going to benchmark multiple classification learners on the task
The benchmark will use the previously defined 10-fold cross-validation.
```{r}
# experiments with learners
design <- benchmark_grid(
  tasks = Boxscores_tsk, 
  learners = lapply(c("classif.log_reg", "classif.kknn", "classif.ranger", "classif.xgboost", "classif.rpart"), lrn), 
  resamplings = rds)

# conduct benchmark
res <- benchmark(design = design)
res

# plot benchmark results
mlr3viz::autoplot(res)

```
The "log.reg" algorithm performs better than the other classification algortithms benchmarked.


Here is a function that predicts win and loss given an input in the form:
c("DR", "FTA", "FTM", "ST", "TO", "X2PA", "X2PM", "X3PA", "X3PM", "RESULT")

As an example, c(28, 20, 14, 8, 12, 50, 25, 24, 8, 1) gives the result below:
```{r}
forecast <- function(x)  {
x <- as.data.frame(t(x))
names(x) <- c("DR", "FTA", "FTM", "ST", "TO", "X2PA", "X2PM", "X3PA", "X3PM", "RESULT")
x
x$RESULT <- as.factor(x$RESULT)
predict(learner, newdata = x, predict_type = "prob")
}
y <- c(30, 20, 14, 8, 12, 50, 25, 24, 8, 1)
forecast(y)
```
