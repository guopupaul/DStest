---
title: "Supervised ML"
output: html_notebook
---
###Using Basketball boxscore elements and machine learning to identify winning team performance targets

##Introduction
The fundamental question facing basketball teams in competition is "what do we have to do to win?". Typically, the coach's philosophy and gameplan, individual team member skills and abilities, overall team offensive performance and overall team defensive performance combine in varying ways to determine who wins. Data Analytics in professional levels of basketball has rapidly grown to quantify these factors in very sophisticated ways. The insights derived are actively used to modify style of play and performance targets to enable more likely wins.

I'm an assitant coach one level below professional basketball. Applying current cutting edge techniques in basketball analytics at my level is problematic for two reasons:
1. the resourcing to collect and analyse the required detailed data in our league is not readily available, and
2. the direct applicability of insights from professional basketball to our league is uncertain.

What is readily available in our league is a detailed boxscore for each game. This will be the data source.

##Aim
To add some rigor to choosing an optimal style of play and setting effective performance targets, this paper will use a set of game boxscores and machine learning techniques to investigate and model key performance targets for a winning game. 

##Method
1. Conduct EDA to identify any high level insights from the boxscores
2. Conduct a logistic regression for a response variable of "WIN/LOSS" as a function of all dependent variables
2. Using PCA, hierarchical clustering, and random forest to reduce the number of dependent variables to avoid overfitting and validate a final model which would be easier to use
4. Calculate threshold values for the key performance targets that the model says will lead to a "WIN" 

##The data
Load the following packages:
```{r message = FALSE}
library(stringr)
library(dplyr)
library(readxl)
library(tidyr)
library(ggplot2)
library("FactoMineR")
library("factoextra")
library("corrplot")
library(pROC)
library(class)
```
Here is the data read in:
```{r}
boxscores <- read_excel("YL1 2019.xlsx", sheet = 2, range = "A1:Y271",col_names = TRUE)
glimpse(boxscores)
```
##Normal
```{r}
library(outliers)
w <- boxscores %>% filter(RESULT == "WIN")
l <- boxscores %>% filter(RESULT == "LOSS")
hist(w$TP,breaks = 30,xlim=c(25,150),ylim=c(0,20),col="gray")
qqnorm(w$TP)
qqline(w$TP)
hist(l$TP,breaks = 30,xlim=c(25,150),ylim=c(0,20),col="gray")
qqnorm(l$TP)
qqline(l$TP)
summary(w$TP)
summary(l$TP)
w_quartiles <- quantile(w$TP)
upper_th <- w_quartiles[4] + 
   1.5 * (w_quartiles[4] - w_quartiles[2])
houtliers <- sort(unique(w$TP[w$TP > upper_th]))
houtliers
l_quartiles <- quantile(l$TP)
lower_th <- l_quartiles[2] - 
   1.5 * (l_quartiles[4] - l_quartiles[2])
loutliers <- sort(unique(l$TP[w$TP < lower_th]))
loutliers
```
the above shows there are no statistically significant outliers in the data.

##Data pre-processing
To prepare a data frame with required numeric variables for analysis:
```{r}
#remove unneeded categorical variables and aggregate variables
boxscores.trim <- boxscores %>% select(-c(RND, TEAM, "3P%", "2P%", "FT%", TR, PTOS, PITP, "2CPS", MIN, TP)) %>%
#convert RESULT to a binomial variable
mutate(RESULT = ifelse(RESULT == "WIN", 1, 0))
#coerce to  a numeric data frame
boxscores.trim <- sapply(boxscores.trim, as.numeric)
boxscores.trim
```



##Feature selection
##Principal Component Analysis

```{r}
# remove response variable
boxscores.pca <- boxscores.trim[, 1:13]
res.pca <- PCA(boxscores.pca, graph = FALSE)
#note that the function 'PCA' automatically standardises the data
eig.val <- get_eigenvalue(res.pca)
eig.val
```
An eigenvalue > 1 indicates that PCs account for more variance than accounted by one of the original variables in standardized data. This is commonly used as a cutoff point for which PCs are retained. This holds true only when the data are standardized.

You can also limit the number of component to that number that accounts for a certain fraction of the total variance. For example, if you are satisfied with 70% of the total variance explained then use the number of components to achieve that.

In this case, 5 dimensions > 1 and cover and cover 70% of the variance.
An alternative method to determine the number of principal components is to look at a Scree Plot, which is the plot of eigenvalues ordered from largest to the smallest. The number of component is determined at the point, beyond which the remaining eigenvalues are all relatively small and of comparable size
```{r}
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 25))
```
5 dimensions still looks right.
The quality of representation of the variables on factor map is called cos2 (square cosine, squared coordinates). You can visualize the cos2 of variables on all the dimensions using the corrplot package:
```{r}
var <- get_pca_var(res.pca)
var
corrplot(var$cos2, is.corr=FALSE)
```
Itâ€™s possible to use the function corrplot() [corrplot package] to highlight the most contributing variables for each dimension:
```{r}
corrplot(var$contrib, is.corr=FALSE) 
#a visualisation
fviz_contrib(res.pca, choice = "var", axes = 1:5, top = 13)
```
The red dashed line on the graph above indicates the expected average contribution. If the contribution of the variables were uniform, the expected value would be 1/length(variables) = 1/13. For a given component, a variable with a contribution larger than this cutoff could be considered as important in contributing to the component.

*************************************************************
#define training(75%) and test(25%) sets
```{r}
bound <- floor((nrow(boxscores.trim)/4)*3) 
#sample rows
boxscores.trim <- boxscores.trim[sample(nrow(boxscores.trim)), ] 
#get training set
boxscores.trim.train <- boxscores.trim[1:bound, ]              
boxscores.trim.test <- boxscores.trim[(bound+1):nrow(boxscores.trim), ]
```

```{r}
#test model
boxscores.trim.test$win_prob <- predict(boxscores_model, newdata = boxscores.trim.test, type = "response")
boxscores.trim.test$win_pred <- ifelse(boxscores.trim.test$win_prob > 0.5, 1, 0)
fit<- boxscores.trim.test$win_pred
actual <- boxscores.trim.test$RESULT
as.matrix(table(actual, fit))
mean(boxscores.trim.test$win_pred == boxscores.trim.test$RESULT)
```

##Using hierarchical clustering
```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
#fetch data
boxscores.hc <- boxscores.active %>% select(-RESULT)
boxscores.hc <- scale(boxscores.hc)


```


##Using the knn algorithm
```{r}
boxscores <- read_excel("YL1 2019.xlsx", sheet = 2, range = "A1:Y271",col_names = TRUE)
glimpse(boxscores)
```
To prepare a tibble with required variables for predictive analysis:
```{r}
boxscores.active <- boxscores %>% select(c("3P%", "2P%", TO, OR, FTA, RESULT))
boxscores.active <- boxscores.active %>% rename(Three_Percent = "3P%", Two_Percent = "2P%")
#calculate a k value
indxTrain <- createDataPartition(y = boxscores.active$RESULT, p = 0.75, list = FALSE)
training <- boxscores.active[indxTrain,]
testing <- boxscores.active[-indxTrain,]
set.seed(400)
ctrl <- trainControl(method = "repeatedcv", repeats = 3)
knnfit<- train(RESULT ~., data = training, method = "knn", trControl = ctrl, preProcess = c("center", "scale"), tuneLength =20)
knnfit
#build model using k = 23
knn(train = boxscores.active[1:5], test = test, cl = boxscores.active$RESULT, k = 23)
#model accuracy
fit <- knn(train = boxscores.active[1:5], test = boxscores.active[1:5], cl = boxscores.active$RESULT, k = 23)
actual <- boxscores.active$RESULT
table(actual, fit)
cm <- as.matrix(table(actual, fit))
sum(diag(cm))/length(actual)
```

```{r}
#define % of training and test set
bound <- floor((nrow(boxscores.active)/4)*3) 
#sample rows
boxscores.active <- boxscores.active[sample(nrow(boxscores.active)), ] 
#get training set
boxscores.active.train <- boxscores.active[1:bound, ]              
boxscores.active.test <- boxscores.active[(bound+1):nrow(boxscores.active), ]
```
